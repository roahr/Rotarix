{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayashre/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.stats import zscore\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(max_features=100)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def preprocess(self, logs):\n",
    "        \"\"\"Process raw logs into structured features\"\"\"\n",
    "        # Extract text features from log messages\n",
    "        text_features = self.tfidf.fit_transform(logs['message']).toarray()\n",
    "        \n",
    "        # Numerical features\n",
    "        num_features = np.column_stack([\n",
    "            logs['failed_attempts'],\n",
    "            logs['success_attempts'],\n",
    "            zscore(logs['latency_ms']),\n",
    "            logs['geolocation_risk'],\n",
    "            logs['privilege_level']\n",
    "        ])\n",
    "        \n",
    "        # Temporal features (moving averages)\n",
    "        window_size = 5\n",
    "        logs['rolling_failures'] = logs['failed_attempts'].rolling(window=window_size).mean()\n",
    "        temporal_features = logs[['rolling_failures']].values\n",
    "        \n",
    "        # Combine all features\n",
    "        features = np.concatenate([text_features, num_features, temporal_features], axis=1)\n",
    "        return self.scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreatDetector:\n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'isolation_forest': IsolationForest(n_estimators=100, contamination=0.05),\n",
    "            'random_forest': RandomForestClassifier(n_estimators=150),\n",
    "            'xgboost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "            'lstm': self._build_lstm_model()\n",
    "        }\n",
    "        self.preprocessor = LogPreprocessor()\n",
    "        \n",
    "    def _build_lstm_model(self):\n",
    "        \"\"\"LSTM for sequential log analysis\"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(64, input_shape=(10, 15), return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(0.001), loss='binary_crossentropy')\n",
    "        return model\n",
    "    \n",
    "    def train(self, X, y=None):\n",
    "        \"\"\"Train both supervised and unsupervised models\"\"\"\n",
    "        # Preprocess data\n",
    "        X_processed = self.preprocessor.preprocess(X)\n",
    "        \n",
    "        # Train unsupervised models\n",
    "        self.models['isolation_forest'].fit(X_processed)\n",
    "        \n",
    "        if y is not None:  # Supervised training\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2)\n",
    "            \n",
    "            # Train Random Forest\n",
    "            self.models['random_forest'].fit(X_train, y_train)\n",
    "            print(f\"Random Forest Accuracy: {self.models['random_forest'].score(X_val, y_val):.2f}\")\n",
    "            \n",
    "            # Train XGBoost\n",
    "            self.models['xgboost'].fit(X_train, y_train)\n",
    "            print(f\"XGBoost Accuracy: {self.models['xgboost'].score(X_val, y_val):.2f}\")\n",
    "            \n",
    "            # Train LSTM (requires sequential data)\n",
    "            X_seq = self._create_sequences(X_processed)\n",
    "            y_seq = y[len(y)-X_seq.shape[0]:]  # Align labels\n",
    "            self.models['lstm'].fit(X_seq, y_seq, epochs=10, batch_size=32)\n",
    "    \n",
    "    def _create_sequences(self, data, seq_length=10):\n",
    "        \"\"\"Convert tabular data to sequences for LSTM\"\"\"\n",
    "        sequences = []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            sequences.append(data[i:i+seq_length])\n",
    "        return np.array(sequences)\n",
    "    \n",
    "    def predict_risk(self, new_logs):\n",
    "        \"\"\"Generate threat risk scores (0-1)\"\"\"\n",
    "        X = self.preprocessor.preprocess(new_logs)\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        results = {\n",
    "            'isolation': self.models['isolation_forest'].decision_function(X),\n",
    "            'rf_proba': self.models['random_forest'].predict_proba(X)[:,1],\n",
    "            'xgb_proba': self.models['xgboost'].predict_proba(X)[:,1],\n",
    "        }\n",
    "        \n",
    "        # LSTM prediction (if sequential data available)\n",
    "        if len(X) >= 10:\n",
    "            X_seq = self._create_sequences(X[-10:])  # Last 10 events\n",
    "            results['lstm_proba'] = self.models['lstm'].predict(X_seq[-1:])[0][0]\n",
    "        \n",
    "        # Calculate composite risk score (weighted average)\n",
    "        weights = {'isolation': 0.3, 'rf_proba': 0.25, 'xgb_proba': 0.25, 'lstm_proba': 0.2}\n",
    "        risk_score = np.mean([\n",
    "            weights['isolation'] * self._normalize(results['isolation']),\n",
    "            weights['rf_proba'] * results['rf_proba'],\n",
    "            weights['xgb_proba'] * results['xgb_proba'],\n",
    "            weights.get('lstm_proba', 0) * results.get('lstm_proba', 0)\n",
    "        ])\n",
    "        \n",
    "        return float(risk_score)\n",
    "    \n",
    "    def _normalize(self, scores):\n",
    "        \"\"\"Scale isolation forest scores to 0-1 range\"\"\"\n",
    "        return (scores - (-0.5)) / (0.5 - (-0.5))\n",
    "    \n",
    "    def save_models(self, path='models/'):\n",
    "        \"\"\"Persist trained models\"\"\"\n",
    "        joblib.dump(self.models['isolation_forest'], f'{path}isolation_forest.pkl')\n",
    "        joblib.dump(self.models['random_forest'], f'{path}random_forest.pkl')\n",
    "        joblib.dump(self.models['xgboost'], f'{path}xgboost.pkl')\n",
    "        self.models['lstm'].save(f'{path}lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyMonitor:\n",
    "    def __init__(self, model_path=None):\n",
    "        self.detector = ThreatDetector()\n",
    "        self.threshold = 0.85  # Rotation threshold\n",
    "        self.log_buffer = []\n",
    "        \n",
    "        if model_path:\n",
    "            self.load_models(model_path)\n",
    "    \n",
    "    def load_models(self, path):\n",
    "        \"\"\"Load pre-trained models\"\"\"\n",
    "        self.detector.models['isolation_forest'] = joblib.load(f'{path}isolation_forest.pkl')\n",
    "        self.detector.models['random_forest'] = joblib.load(f'{path}random_forest.pkl')\n",
    "        self.detector.models['xgboost'] = joblib.load(f'{path}xgboost.pkl')\n",
    "    \n",
    "    def ingest_logs(self, logs):\n",
    "        \"\"\"Process incoming logs in real-time\"\"\"\n",
    "        self.log_buffer.extend(logs)\n",
    "        \n",
    "        # Keep last 1000 logs to prevent memory overload\n",
    "        if len(self.log_buffer) > 1000:\n",
    "            self.log_buffer = self.log_buffer[-1000:]\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        log_df = pd.DataFrame(self.log_buffer)\n",
    "        \n",
    "        # Calculate risk score\n",
    "        risk_score = self.detector.predict_risk(log_df)\n",
    "        \n",
    "        # Determine action\n",
    "        action = \"key_rotation\" if risk_score >= self.threshold else \"monitor\"\n",
    "        \n",
    "        return {\n",
    "            \"risk_score\": risk_score,\n",
    "            \"action\": action,\n",
    "            \"top_features\": self._get_risk_factors(log_df)\n",
    "        }\n",
    "    \n",
    "    def _get_risk_factors(self, logs):\n",
    "        \"\"\"Identify highest-contributing risk factors\"\"\"\n",
    "        processed = self.detector.preprocessor.preprocess(logs)\n",
    "        \n",
    "        # Get feature importances from Random Forest\n",
    "        importances = self.detector.models['random_forest'].feature_importances_\n",
    "        top_indices = np.argsort(importances)[-3:][::-1]  # Top 3 features\n",
    "        \n",
    "        # Map back to feature names (simplified)\n",
    "        features = ['login_failures', 'geo_risk', 'privilege_escalation', \n",
    "                   'latency', 'message_tfidf']  # Extend based on actual features\n",
    "        return {features[i]: float(importances[i]) for i in top_indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simulate training data (replace with real logs)\n",
    "training_data = pd.DataFrame({\n",
    "    'message': [\"Failed login\", \"SSH brute force\", \"Login success\"]*100,\n",
    "    'failed_attempts': np.random.randint(0, 5, 300),\n",
    "    'success_attempts': np.random.randint(0, 2, 300),\n",
    "    'latency_ms': np.random.normal(200, 50, 300),\n",
    "    'geolocation_risk': np.random.uniform(0, 1, 300),\n",
    "    'privilege_level': np.random.choice([0, 1, 2], 300)\n",
    "})\n",
    "labels = np.random.randint(0, 2, 300)  # Mock labels (0=normal, 1=threat)\n",
    "\n",
    "# 2. Train models\n",
    "detector = ThreatDetector()\n",
    "detector.train(training_data, labels)\n",
    "detector.save_models()\n",
    "\n",
    "# 3. Real-time monitoring\n",
    "monitor = AnomalyMonitor(model_path='models/')\n",
    "\n",
    "new_logs = [{\n",
    "    'message': \"Repeated failed login attempts from unusual IP\",\n",
    "    'failed_attempts': 8,\n",
    "    'success_attempts': 0,\n",
    "    'latency_ms': 1200,\n",
    "    'geolocation_risk': 0.9,\n",
    "    'privilege_level': 2\n",
    "}]\n",
    "\n",
    "result = monitor.ingest_logs(new_logs)\n",
    "print(f\"Risk Score: {result['risk_score']:.2f} â†’ Action: {result['action'].upper()}\")\n",
    "print(\"Key Risk Factors:\", result['top_features'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
